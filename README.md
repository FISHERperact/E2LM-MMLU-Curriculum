# NeurIPS-2025-E2LM-Competition-Early-Training-Evaluation-of-Language-Models-our-solution-
We propose MMLU-Curriculum for the E2LM challenge, a curriculum-based benchmark that ranks MMLU tasks by difficulty using small model performance, assigns easier tasks early and harder ones later, and converts MCQs to cloze-style generation for smoother, more discriminative evaluation.

# SmolLM2-1.7B æ¨¡å‹è¯„ä¼°é¡¹ç›®ï¼ˆåç»­éšæ„æ›´æ”¹å…¶ä»–æ¨¡å‹ï¼‰

> **ä¸€å¥è¯æ€»ç»“**: ä½¿ç”¨ lm-evaluation-harness è¯„ä¼° SmolLM2-1.7B æ¨¡å‹åœ¨ 8 ä¸ªæ ‡å‡†åŸºå‡†æµ‹è¯•ä¸Šçš„æ€§èƒ½

**è¯„ä¼°å®Œæˆæ—¶é—´**: 2025-10-19 12:36  
**æ€»é¢˜ç›®æ•°**: ~34,544 é¢˜  
**è¯„ä¼°ä»»åŠ¡**: 8 ä¸ªä¸»ä»»åŠ¡ + 61 ä¸ª MMLU å­ä»»åŠ¡

---

## âš ï¸ é‡è¦ä¾èµ–

**æœ¬é¡¹ç›®ä¾èµ–äº [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)ï¼Œå¿…é¡»å…ˆå®‰è£…ï¼**

### å¿«é€Ÿå®‰è£…

```bash
# 1. å…‹éš† lm-evaluation-harness
cd /home2/yth  # æˆ–æ‚¨çš„å·¥ä½œç›®å½•
git clone https://github.com/tiiuae/lm-evaluation-harness-competition
cd lm-evaluation-harness-competition

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒå¹¶å®‰è£…
python3 -m venv .venv
source .venv/bin/activate
pip install -e ".[dev]" -i https://pypi.tuna.tsinghua.edu.cn/simple  # ä½¿ç”¨æ¸…åé•œåƒåŠ é€Ÿ

# 3. éªŒè¯å®‰è£…
lm_eval --help
```

**æ³¨æ„**: å¦‚æœæ‚¨å°† `lm-evaluation-harness-competition` å®‰è£…åˆ°å…¶ä»–ä½ç½®ï¼Œéœ€è¦ä¿®æ”¹ `run_evaluation.sh` ä¸­çš„ `LM_EVAL_DIR` å˜é‡ã€‚

---

## ğŸ“Š SmlLLM2è¯„ä¼°ç»“æœæ¦‚è§ˆ

| æ•°æ®é›† | é¢˜ç›®æ•° | å‡†ç¡®ç‡ | å½’ä¸€åŒ–å‡†ç¡®ç‡ | è¯„çº§ |
|--------|--------|--------|--------------|------|
| **PIQA** | 1,838 | 74.65% | 74.81% | â­â­â­â­â­ |
| **ARC-Easy** | 2,376 | 72.73% | 69.70% | â­â­â­â­â­ |
| **BoolQ** | 3,270 | 62.94% | 62.94% | â­â­â­â­ |
| **HellaSwag** | 10,042 | 46.92% | 61.96% | â­â­â­â­ |
| **WinoGrande** | 1,267 | 60.85% | 60.85% | â­â­â­â­ |
| **ARC-Challenge** | 1,172 | 37.63% | 40.27% | â­â­â­ |
| **OpenBookQA** | 500 | 28.00% | 39.40% | â­â­â­ |
| **MMLU** | ~14,079 | 23.62% | 23.62% | â­â­ |

**æ¨¡å‹ä¼˜åŠ¿**: ç‰©ç†å¸¸è¯† (74.65%)ã€åŸºç¡€ç§‘å­¦ (72.73%)ã€é˜…è¯»ç†è§£ (62.94%)

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. æŸ¥çœ‹è¯„ä¼°ç»“æœ

```bash
cd /home2/yth/smollm2_evaluation
 
# æŸ¥çœ‹ç»“æœ JSON
cat results/step-125000/HuggingFaceTB__SmolLM2-1.7B-intermediate-checkpoints/results_*.json | python3 -m json.tool

# æˆ–ä½¿ç”¨è„šæœ¬æŸ¥çœ‹
python3 view_questions.py hellaswag --stats
```

### 2. æŸ¥çœ‹å…·ä½“é¢˜ç›®

```bash
# æŸ¥çœ‹ 5 ä¸ª HellaSwag é¢˜ç›®
python3 view_questions.py hellaswag 5

# æŸ¥çœ‹ 3 ä¸ª ARC-Challenge é¢˜ç›®
python3 view_questions.py arc_challenge 3

# æŸ¥çœ‹ä»»ä½•ä»»åŠ¡çš„ç»Ÿè®¡
python3 view_questions.py mmlu_anatomy --stats
```

### 3. é‡æ–°è¿è¡Œè¯„ä¼°ï¼ˆå¦‚éœ€è¦ï¼‰

```bash
# æ¿€æ´»ç¯å¢ƒ
cd /home2/yth/lm-evaluation-harness-competition
source .venv/bin/activate

# è¿è¡Œè¯„ä¼°ï¼ˆä½¿ç”¨é•œåƒåŠ é€Ÿï¼‰
cd /home2/yth/smollm2_evaluation
bash run_evaluation.sh
```

---

## ğŸ“‚ é¡¹ç›®ç»“æ„

```
smollm2_evaluation/
â”‚
â”œâ”€â”€ ğŸ“Š è¯„ä¼°ç»“æœ
â”‚   â””â”€â”€ results/step-125000/
â”‚       â”œâ”€â”€ results_*.json                    # æ€»ç»“æœï¼ˆå‡†ç¡®ç‡ç­‰ï¼‰
â”‚       â””â”€â”€ samples_*.jsonl (69ä¸ªæ–‡ä»¶)        # æ‰€æœ‰é¢˜ç›® + æ¨¡å‹ç­”æ¡ˆ
â”‚
â”œâ”€â”€ ğŸ”§ è„šæœ¬å·¥å…·
â”‚   â”œâ”€â”€ view_questions.py                     # æŸ¥çœ‹é¢˜ç›®ï¼ˆæ¨èä½¿ç”¨ï¼ï¼‰
â”‚   â”œâ”€â”€ run_evaluation.sh                     # è¿è¡Œè¯„ä¼°
â”‚   â”œâ”€â”€ create_submission.sh                  # åˆ›å»ºæäº¤æ–‡ä»¶
â”‚   â””â”€â”€ plot_results.py                       # ç»“æœå¯è§†åŒ–
â”‚
â””â”€â”€ ğŸ“– æ–‡æ¡£
    â”œâ”€â”€ README.md                             # æœ¬æ–‡ä»¶ï¼ˆä¸»å…¥å£ï¼‰
    â””â”€â”€ GUIDE.md                              # è¯¦ç»†æŒ‡å—ï¼ˆå¯é€‰é˜…è¯»ï¼‰
```

---

## ğŸ¯ æ ¸å¿ƒä»»åŠ¡è¯´æ˜

### æ‚¨åœ¨åšä»€ä¹ˆï¼Ÿ

**ç±»æ¯”**: åƒæ˜¯ç»™ AI å­¦ç”Ÿï¼ˆSmolLM2ï¼‰è¿›è¡Œå…¨é¢è€ƒè¯•

```
è¾“å…¥: SmolLM2-1.7B æ¨¡å‹ (checkpoint: step-125000)
  â†“
å¤„ç†: åœ¨ 8 ä¸ªæ ‡å‡†æµ‹è¯•ä¸Šè¿è¡Œ
  â€¢ HellaSwag     - å¸¸è¯†æ¨ç†
  â€¢ ARC           - ç§‘å­¦é—®ç­”
  â€¢ MMLU          - 57 ä¸ªå­¦ç§‘çŸ¥è¯†
  â€¢ PIQA          - ç‰©ç†å¸¸è¯†
  â€¢ WinoGrande    - è¯­è¨€ç†è§£
  â€¢ OpenBookQA    - å¼€æ”¾é—®ç­”
  â€¢ BoolQ         - æ˜¯éåˆ¤æ–­
  â†“
è¾“å‡º: æ€§èƒ½æŒ‡æ ‡ï¼ˆå‡†ç¡®ç‡ã€æ ‡å‡†è¯¯å·®ç­‰ï¼‰
```

### æ•°æ®ä»å“ªé‡Œæ¥ï¼Ÿ

âœ… **æ‰€æœ‰æ•°æ®æ¥è‡ª HuggingFace Hub**ï¼ˆè‡ªåŠ¨ä¸‹è½½ï¼‰

| æ•°æ®é›† | HuggingFace åœ°å€ |
|--------|------------------|
| HellaSwag | `hellaswag` |
| ARC | `allenai/ai2_arc` |
| MMLU | `cais/mmlu` |
| PIQA | `baber/piqa` |
| WinoGrande | `winogrande` |
| OpenBookQA | `openbookqa` |
| BoolQ | `boolq` |

**ç¼“å­˜ä½ç½®**:
- æ¨¡å‹: `~/.cache/huggingface/hub/`
- æ•°æ®é›†: `~/.cache/huggingface/datasets/`

---

## ğŸ“ é¢˜ç›®åœ¨å“ªé‡Œï¼Ÿ

### ä¸»è¦ä½ç½®ï¼ˆæ¨èæŸ¥çœ‹ï¼‰â­

```bash
/home2/yth/smollm2_evaluation/results/step-125000/
  HuggingFaceTB__SmolLM2-1.7B-intermediate-checkpoints/
    â””â”€â”€ samples_*.jsonl  (69ä¸ªæ–‡ä»¶)
```

**æ–‡ä»¶åˆ—è¡¨**:
```
samples_hellaswag_*.jsonl          â†’ 10,042 é¢˜
samples_arc_easy_*.jsonl           â†’ 2,376 é¢˜
samples_arc_challenge_*.jsonl      â†’ 1,172 é¢˜
samples_piqa_*.jsonl               â†’ 1,838 é¢˜
samples_winogrande_*.jsonl         â†’ 1,267 é¢˜
samples_openbookqa_*.jsonl         â†’ 500 é¢˜
samples_boolq_*.jsonl              â†’ 3,270 é¢˜
samples_mmlu_*_*.jsonl (57ä¸ª)      â†’ ~14,079 é¢˜
```

### æŸ¥çœ‹æ–¹æ³•

#### æ–¹æ³• 1: ä½¿ç”¨è„šæœ¬ï¼ˆæœ€ç®€å•ï¼ï¼‰

```bash
cd /home2/yth/smollm2_evaluation

# æŸ¥çœ‹é¢˜ç›®ï¼ˆå¸¦æ ¼å¼åŒ–è¾“å‡ºï¼‰
python3 view_questions.py hellaswag 3

# è¾“å‡ºç¤ºä¾‹:
# ======================================================================
# é¢˜ç›® #1
# ======================================================================
# ğŸ“ é—®é¢˜: A glass of cold water is set on a desktop...
# ğŸ“‹ é€‰é¡¹:
#    ğŸ¤– [A] some water dripped over the side.  
#       [B] the water soaked through the glass.  
#       [C] water vapor condensed on the sides. âœ“
#       [D] someone sprayed the glass with water.  
# âœ… æ­£ç¡®ç­”æ¡ˆ: 2 (C)
# ğŸ¤– æ¨¡å‹ç­”æ¡ˆ: 0 (A)
# âŒ ç»“æœ: ç­”é”™äº†
```

#### æ–¹æ³• 2: ç›´æ¥æŸ¥çœ‹ JSON

```bash
cd results/step-125000/HuggingFaceTB__SmolLM2-1.7B-intermediate-checkpoints/

# æŸ¥çœ‹ç¬¬ä¸€ä¸ªé¢˜ç›®ï¼ˆæ ¼å¼åŒ–ï¼‰
head -1 samples_hellaswag_*.jsonl | python3 -m json.tool

# åœ¨ç¼–è¾‘å™¨ä¸­æ‰“å¼€
nano samples_hellaswag_*.jsonl
```

#### æ–¹æ³• 3: ç»Ÿè®¡åˆ†æ

```bash
# ç»Ÿè®¡é¢˜ç›®æ€»æ•°
wc -l samples_*.jsonl

# æŸ¥çœ‹ä»»åŠ¡ç»Ÿè®¡
python3 view_questions.py hellaswag --stats
```

### é¢˜ç›®æ–‡ä»¶æ ¼å¼

æ¯ä¸ª `.jsonl` æ–‡ä»¶åŒ…å«å¤šè¡Œï¼Œæ¯è¡Œæ˜¯ä¸€ä¸ª JSON å¯¹è±¡ï¼ˆä¸€ä¸ªé¢˜ç›®ï¼‰ï¼š

```json
{
  "doc_id": 0,
  "doc": {
    "question": "é¢˜ç›®æ–‡æœ¬",
    "choices": {"text": ["A", "B", "C", "D"]},
    "answerKey": "C"
  },
  "target": "2",                           // æ­£ç¡®ç­”æ¡ˆç´¢å¼•
  "resps": [[...], [...], [...], [...]],  // æ¨¡å‹å¯¹æ¯ä¸ªé€‰é¡¹çš„è¯„åˆ†
  "filtered_resps": [[-18.875], [-25.125], [-21.875], [-15.250]]
}
```

**å…³é”®å­—æ®µ**:
- `doc`: åŸå§‹é¢˜ç›®ï¼ˆé—®é¢˜ã€é€‰é¡¹ã€æ­£ç¡®ç­”æ¡ˆï¼‰
- `target`: æ­£ç¡®ç­”æ¡ˆçš„ç´¢å¼•ï¼ˆ0, 1, 2, 3ï¼‰
- `filtered_resps`: æ¨¡å‹åˆ†æ•°ï¼ˆè¶Šé«˜ = è¶Šå¯èƒ½é€‰æ‹©ï¼‰

---

## ğŸ“ è¯„ä¼°ä»»åŠ¡è¯¦è§£

### æ•°æ®é›†è¯´æ˜

#### 1. HellaSwagï¼ˆå¸¸è¯†æ¨ç†ï¼‰âœ…
- **é¢˜ç›®æ•°**: 10,042
- **ç±»å‹**: æƒ…å¢ƒç»­å†™
- **ç¤ºä¾‹**: "ä¸€ä¸ªäººååœ¨å±‹é¡¶ä¸Šã€‚ä»–..." â†’ é€‰æ‹©æœ€åˆç†çš„åç»­
- **æ‚¨çš„è¡¨ç°**: 61.96% (å½’ä¸€åŒ–)

#### 2. ARCï¼ˆç§‘å­¦é—®ç­”ï¼‰âœ…
- **ARC-Easy**: 2,376 é¢˜ï¼Œå°å­¦åˆ°åˆä¸­æ°´å¹³ï¼Œ**72.73%** â­â­â­â­â­
- **ARC-Challenge**: 1,172 é¢˜ï¼Œé«˜ä¸­æ°´å¹³ï¼Œ40.27%
- **ç±»å‹**: å¤šé¡¹é€‰æ‹©ç§‘å­¦é¢˜
- **ç¤ºä¾‹**: "å“ªç§èƒ½æºåˆ©ç”¨é‡åŠ›ï¼Ÿ" â†’ A. æ½®æ±èƒ½å’Œæ°´èƒ½

#### 3. MMLUï¼ˆå¤šå­¦ç§‘ç†è§£ï¼‰âœ…
- **é¢˜ç›®æ•°**: ~14,079ï¼ˆ57 ä¸ªå­¦ç§‘ï¼‰
- **å­¦ç§‘**: æ•°å­¦ã€ç‰©ç†ã€åŒ–å­¦ã€ç”Ÿç‰©ã€å†å²ã€æ³•å¾‹ã€åŒ»å­¦ç­‰
- **ç±»å‹**: 4 é€‰ 1 å¤šé¡¹é€‰æ‹©
- **æ‚¨çš„è¡¨ç°**: 23.62%ï¼ˆæ¥è¿‘éšæœº 25%ï¼Œè¿™å¯¹å°æ¨¡å‹æ˜¯æ­£å¸¸çš„ï¼‰

#### 4. PIQAï¼ˆç‰©ç†å¸¸è¯†ï¼‰âœ…
- **é¢˜ç›®æ•°**: 1,838
- **ç±»å‹**: é€‰æ‹©å®ç°ç›®æ ‡çš„åˆç†æ–¹æ³•
- **ç¤ºä¾‹**: "å¦‚ä½•åœ¨å®¶åšå†°æ·‡æ·‹ï¼Ÿ" â†’ A. æ”¾å†°ç®±è¿‡å¤œ
- **æ‚¨çš„è¡¨ç°**: **74.65%** â­â­â­â­â­ï¼ˆæœ€ä½³ï¼ï¼‰

#### 5. WinoGrandeï¼ˆè¯­è¨€ç†è§£ï¼‰âœ…
- **é¢˜ç›®æ•°**: 1,267
- **ç±»å‹**: ä»£è¯æ¶ˆæ­§å’Œå¡«ç©º
- **ç¤ºä¾‹**: "The trophy doesn't fit in the suitcase because ___ is too large."
- **æ‚¨çš„è¡¨ç°**: 60.85%

#### 6. OpenBookQAï¼ˆå¼€æ”¾é—®ç­”ï¼‰âœ…
- **é¢˜ç›®æ•°**: 500
- **ç±»å‹**: éœ€è¦æ¨ç†çš„ç§‘å­¦é—®ç­”
- **æ‚¨çš„è¡¨ç°**: 39.40% (å½’ä¸€åŒ–)

#### 7. BoolQï¼ˆæ˜¯éåˆ¤æ–­ï¼‰âœ…
- **é¢˜ç›®æ•°**: 3,270
- **ç±»å‹**: åŸºäºæ®µè½å›ç­” True/False
- **æ‚¨çš„è¡¨ç°**: 62.94%

---

## ğŸ”§ è‡ªå®šä¹‰è¯„ä¼°

### è¯„ä¼°å•ä¸ªä»»åŠ¡

```bash
cd /home2/yth/lm-evaluation-harness-competition
source .venv/bin/activate

lm_eval --model hf \
    --model_args pretrained=HuggingFaceTB/SmolLM2-1.7B-intermediate-checkpoints,revision=step-125000,dtype=bfloat16 \
    --tasks hellaswag \
    --batch_size auto \
    --output_path results/single_task/
```

### è¯„ä¼°å¤šä¸ª checkpoint

ç¼–è¾‘ `run_evaluation.sh` ä¸­çš„ `CHECKPOINTS` æ•°ç»„ï¼š

```bash
CHECKPOINTS=(
    "step-100000"
    "step-125000"  # å·²å®Œæˆ
    "step-150000"
)

# é‡æ–°è¿è¡Œ
bash run_evaluation.sh
```

### å¿«é€Ÿæµ‹è¯•ï¼ˆé™åˆ¶æ ·æœ¬æ•°ï¼‰

```bash
lm_eval --model hf \
    --model_args pretrained=HuggingFaceTB/SmolLM2-1.7B-intermediate-checkpoints,revision=step-125000 \
    --tasks hellaswag \
    --limit 10 \
    --output_path results/quick_test/
```

---

## ğŸ“ˆ ç»“æœå¯è§†åŒ–

### ç”Ÿæˆå›¾è¡¨

```bash
python plot_results.py
```

ç”Ÿæˆæ–‡ä»¶:
- `results_comparison.png` - å„ä»»åŠ¡æ€§èƒ½å¯¹æ¯”
- `average_performance.png` - å¹³å‡æ€§èƒ½è¶‹åŠ¿

---

## ğŸ“¦ åˆ›å»ºæäº¤æ–‡ä»¶

```bash
# åˆ›å»º submission.zip
bash create_submission.sh --include-results

# éªŒè¯
unzip -l submission.zip
```

**æäº¤å†…å®¹**:
```
submission.zip
â”œâ”€â”€ evaluation.patch      # Git diffï¼ˆå¦‚æœ‰ä»£ç ä¿®æ”¹ï¼‰
â”œâ”€â”€ metadata.yaml         # è¯„ä¼°å…ƒæ•°æ®
â””â”€â”€ results/              # è¯„ä¼°ç»“æœï¼ˆå¯é€‰ï¼‰
```

---

## ğŸ’¡ è¯„ä¼°æŒ‡æ ‡è¯´æ˜

### å‡†ç¡®ç‡ç±»å‹

- **acc (Accuracy)**: åŸå§‹å‡†ç¡®ç‡ = æ­£ç¡®æ•° / æ€»é¢˜æ•°
- **acc_norm (Normalized Accuracy)**: å½’ä¸€åŒ–å‡†ç¡®ç‡ï¼ˆæ¨èä½¿ç”¨ï¼ï¼‰

### ä¸ºä»€ä¹ˆä½¿ç”¨å½’ä¸€åŒ–å‡†ç¡®ç‡ï¼Ÿ

**é—®é¢˜**: å¤š token ç­”æ¡ˆä¼šå—æƒ©ç½š

```
"Abu Dhabi" â†’ 2 tokens: P(Abu) Ã— P(Dhabi|Abu)
"Dubai"     â†’ 1 token:  P(Dubai)

å› ä¸ºæ¦‚ç‡ â‰¤ 1ï¼Œå¤š token ç­”æ¡ˆçš„æ¦‚ç‡ä¼šæ›´ä½
```

**è§£å†³**: å½’ä¸€åŒ– = å¹³å‡æ¯ä¸ª token çš„å¯¹æ•°æ¦‚ç‡

```
acc_norm = log(æ¦‚ç‡) / tokenæ•°é‡
```

è¿™æ ·å¯ä»¥**å…¬å¹³æ¯”è¾ƒä¸åŒé•¿åº¦çš„ç­”æ¡ˆ**ã€‚

---

## ğŸ› ï¸ å¸¸ç”¨å‘½ä»¤

### æŸ¥çœ‹é¢˜ç›®

```bash
# æŸ¥çœ‹é¢˜ç›®ï¼ˆæ¨èï¼ï¼‰
python3 view_questions.py hellaswag 5
python3 view_questions.py arc_challenge 3
python3 view_questions.py mmlu_anatomy 10

# æŸ¥çœ‹ç»Ÿè®¡
python3 view_questions.py hellaswag --stats

# åˆ—å‡ºæ‰€æœ‰å¯ç”¨ä»»åŠ¡
python3 view_questions.py
```

### æŸ¥çœ‹ç»“æœ

```bash
# è¿›å…¥ç»“æœç›®å½•
cd results/step-125000/HuggingFaceTB__SmolLM2-1.7B-intermediate-checkpoints/

# æŸ¥çœ‹æ‰€æœ‰æ ·æœ¬æ–‡ä»¶
ls -lh samples_*.jsonl

# ç»Ÿè®¡é¢˜ç›®æ€»æ•°
wc -l samples_*.jsonl

# æŸ¥çœ‹ç»“æœ JSON
cat results_*.json | python3 -m json.tool | less
```

### ç³»ç»Ÿæ£€æŸ¥

```bash
# æŸ¥çœ‹ GPU
nvidia-smi

# æŸ¥çœ‹ç¼“å­˜å¤§å°
du -sh ~/.cache/huggingface/*

# åˆ—å‡ºå¯ç”¨ä»»åŠ¡
cd /home2/yth/lm-evaluation-harness-competition
source .venv/bin/activate
lm_eval --tasks list | grep -E "hellaswag|arc|mmlu"
```

---

## âš ï¸ å¸¸è§é—®é¢˜

### 1. ä¸‹è½½é€Ÿåº¦æ…¢

**é—®é¢˜**: æ¨¡å‹/æ•°æ®é›†ä¸‹è½½åªæœ‰å‡ å KB/s

**è§£å†³**: ä½¿ç”¨å›½å†…é•œåƒ

```bash
# ä¸´æ—¶è®¾ç½®
export HF_ENDPOINT=https://hf-mirror.com

# æ°¸ä¹…è®¾ç½®
echo 'export HF_ENDPOINT=https://hf-mirror.com' >> ~/.bashrc
source ~/.bashrc
```

### 2. GPU å†…å­˜ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ–¹æ³• 1: å‡å° batch_size
lm_eval ... --batch_size 4

# æ–¹æ³• 2: ä½¿ç”¨æ›´å°çš„æ•°æ®ç±»å‹
lm_eval ... --model_args dtype=float16

# æ–¹æ³• 3: å•ç‹¬è¯„ä¼°ä»»åŠ¡
lm_eval ... --tasks hellaswag  # ä¸€æ¬¡åªè¯„ä¼°ä¸€ä¸ª
```

### 3. æ¨¡å‹æ–‡ä»¶æŸå

**é”™è¯¯**: `SafetensorError`

**è§£å†³**:
```bash
# æ¸…é™¤æŸåç¼“å­˜
rm -rf ~/.cache/huggingface/hub/models--HuggingFaceTB*

# é‡æ–°ä¸‹è½½ï¼ˆä½¿ç”¨é•œåƒï¼‰
export HF_ENDPOINT=https://hf-mirror.com
bash run_evaluation.sh
```

### 4. é‡å¤è­¦å‘Šä¿¡æ¯

**ç°è±¡**: `trust_remote_code is not supported anymore` é‡å¤å¤šæ¬¡

**è§£é‡Š**: 
- è¿™æ˜¯**è­¦å‘Š**ï¼Œä¸æ˜¯é”™è¯¯
- MMLU æœ‰ 57 ä¸ªå­ä»»åŠ¡ï¼Œæ¯ä¸ªéƒ½æ˜¾ç¤ºä¸€æ¬¡
- **ä¸å½±å“è¯„ä¼°ç»“æœ**ï¼Œå¯ä»¥å®‰å…¨å¿½ç•¥

---

## ğŸ“Š æ€§èƒ½åˆ†æ

### æ¨¡å‹ä¼˜åŠ¿é¢†åŸŸ

âœ… **ç‰©ç†å¸¸è¯†** (74.65%)  
âœ… **åŸºç¡€ç§‘å­¦** (72.73%)  
âœ… **é˜…è¯»ç†è§£** (62.94%)  
âœ… **æƒ…å¢ƒæ¨ç†** (61.96%)  
âœ… **è¯­è¨€ç†è§£** (60.85%)  

### å¾…æ”¹è¿›é¢†åŸŸ

âš ï¸ **ä¸“ä¸šçŸ¥è¯†** (23.62%) - MMLU æ¶µç›–å¤§å­¦çº§çŸ¥è¯†ï¼Œå¯¹ 1.7B å°æ¨¡å‹æœ‰æŒ‘æˆ˜  
âš ï¸ **æ·±å±‚æ¨ç†** (39.40%) - OpenBookQA éœ€è¦å¤šæ­¥æ¨ç†  

### ä¸éšæœºçŒœæµ‹å¯¹æ¯”

| æ•°æ®é›† | éšæœºå‡†ç¡®ç‡ | æ‚¨çš„æ¨¡å‹ | æå‡ |
|--------|-----------|---------|------|
| HellaSwag | 25% | **61.96%** | +147% |
| ARC-Easy | 25% | **72.73%** | +191% |
| PIQA | 50% | **74.65%** | +49% |
| BoolQ | 50% | **62.94%** | +26% |

**ç»“è®º**: æ¨¡å‹åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šéƒ½**æ˜æ˜¾ä¼˜äºéšæœºçŒœæµ‹**ï¼

---

## ğŸ”— é‡è¦è·¯å¾„

```bash
# é¡¹ç›®ç›®å½•
/home2/yth/smollm2_evaluation/

# è¯„ä¼°æ¡†æ¶
/home2/yth/lm-evaluation-harness-competition/

# æ¨¡å‹ç¼“å­˜
~/.cache/huggingface/hub/

# æ•°æ®é›†ç¼“å­˜
~/.cache/huggingface/datasets/

# è¯„ä¼°ç»“æœ
/home2/yth/smollm2_evaluation/results/step-125000/
```

---

## ğŸ“š å‚è€ƒèµ„æº

### æ–‡æ¡£

- **README.md** (æœ¬æ–‡ä»¶) - å¿«é€Ÿå…¥å£å’Œå¸¸ç”¨æ“ä½œ
- **GUIDE.md** - è¯¦ç»†æŠ€æœ¯æŒ‡å—ï¼ˆæ·±å…¥äº†è§£æ—¶é˜…è¯»ï¼‰

### å¤–éƒ¨é“¾æ¥

- [LM Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness)
- [SmolLM2 æ¨¡å‹](https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-intermediate-checkpoints)
- [é›†æˆæŒ‡å—](https://huggingface.co/blog/integrating-benchmarks-lm-eval)

### æ•°æ®é›†ä¸»é¡µ

- [HellaSwag](https://rowanzellers.com/hellaswag/)
- [ARC](https://allenai.org/data/arc)
- [MMLU](https://github.com/hendrycks/test)
- [PIQA](https://yonatanbisk.com/piqa/)

---

## ğŸ“ æ€»ç»“

### æ‚¨å®Œæˆäº†ä»€ä¹ˆ

1. âœ… **ç¯å¢ƒæ­å»º** - Python, PyTorch, lm-eval
2. âœ… **æ¨¡å‹è¯„ä¼°** - åœ¨ 8 ä¸ªåŸºå‡†æµ‹è¯•ä¸Šè¿è¡Œ
3. âœ… **ç»“æœç”Ÿæˆ** - è·å¾—äº†è¯¦ç»†çš„æ€§èƒ½æ•°æ®
4. âœ… **é¢˜ç›®ä¿å­˜** - æ‰€æœ‰ ~34,544 ä¸ªé¢˜ç›®éƒ½å·²ä¿å­˜

### å…³é”®æˆæœ

- **æ€§èƒ½æ•°æ®**: 8 ä¸ªä»»åŠ¡çš„å®Œæ•´è¯„ä¼°ç»“æœ
- **é¢˜ç›®åº“**: 34,544 ä¸ªé¢˜ç›® + æ¨¡å‹ç­”æ¡ˆ
- **å¯è§†åŒ–å·¥å…·**: æŸ¥çœ‹å’Œåˆ†æè„šæœ¬
- **å¯å¤ç”¨æ¡†æ¶**: å¯ç”¨äºå…¶ä»–æ¨¡å‹è¯„ä¼°

### ä¸‹ä¸€æ­¥

1. **åˆ†æç»“æœ**: ä½¿ç”¨ `view_questions.py` æŸ¥çœ‹å…·ä½“é¢˜ç›®
2. **ç”Ÿæˆå›¾è¡¨**: è¿è¡Œ `plot_results.py`
3. **åˆ›å»ºæäº¤**: è¿è¡Œ `create_submission.sh`
4. **è¯„ä¼°å…¶ä»–ç‰ˆæœ¬**: ä¿®æ”¹ checkpoint é‡æ–°è¯„ä¼°

---

**é¡¹ç›®åˆ›å»º**: 2025-10-19  
**è¯„ä¼°å®Œæˆ**: 2025-10-19 12:36  
**æ€»è€—æ—¶**: çº¦ 33 åˆ†é’Ÿ

**ğŸ‰ è¯„ä¼°å®Œæˆï¼ç¥æ‚¨ä½¿ç”¨æ„‰å¿«ï¼**

å¦‚æœ‰ç–‘é—®ï¼Œè¯·æŸ¥çœ‹ `GUIDE.md` è·å–è¯¦ç»†è¯´æ˜ã€‚
