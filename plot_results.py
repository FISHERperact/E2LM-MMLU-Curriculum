#!/usr/bin/env python3
"""
ç»“æœå¯è§†åŒ–è„šæœ¬
æ ¹æ®è¯„ä¼°ç»“æœç”Ÿæˆæ€§èƒ½å¯¹æ¯”å›¾è¡¨
"""

import json
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path
from collections import defaultdict


def load_results(results_dir="results"):
    """åŠ è½½æ‰€æœ‰è¯„ä¼°ç»“æœ"""
    results_path = Path(results_dir)
    
    if not results_path.exists():
        print(f"âŒ é”™è¯¯: ç»“æœç›®å½•ä¸å­˜åœ¨: {results_dir}")
        return []
    
    results = []
    
    # éå†æ‰€æœ‰ checkpoint ç›®å½•
    for checkpoint_dir in sorted(results_path.iterdir()):
        if not checkpoint_dir.is_dir():
            continue
        
        # æŸ¥æ‰¾ JSON ç»“æœæ–‡ä»¶
        json_files = list(checkpoint_dir.glob("**/*.json"))
        
        if not json_files:
            print(f"âš ï¸  è­¦å‘Š: {checkpoint_dir.name} ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°ç»“æœæ–‡ä»¶")
            continue
        
        # è¯»å–ç¬¬ä¸€ä¸ª JSON æ–‡ä»¶
        json_file = json_files[0]
        
        try:
            with open(json_file, 'r') as f:
                data = json.load(f)
                results.append({
                    'checkpoint': checkpoint_dir.name,
                    'data': data
                })
                print(f"âœ“ åŠ è½½: {checkpoint_dir.name}")
        except Exception as e:
            print(f"âœ— é”™è¯¯: æ— æ³•è¯»å– {json_file}: {e}")
    
    return results


def plot_comparison(results, output_file="results_comparison.png"):
    """ç»˜åˆ¶æ€§èƒ½å¯¹æ¯”å›¾"""
    
    if not results:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„ç»“æœæ•°æ®")
        return
    
    # æå–æ•°æ®
    checkpoints = []
    task_scores = defaultdict(lambda: {'acc': [], 'acc_norm': []})
    
    for result in results:
        checkpoint = result['checkpoint']
        checkpoints.append(checkpoint)
        
        # æå–æ¯ä¸ªä»»åŠ¡çš„åˆ†æ•°
        if 'results' in result['data']:
            for task_name, metrics in result['data']['results'].items():
                # æå– acc å’Œ acc_norm
                acc = metrics.get('acc,none', metrics.get('acc', None))
                acc_norm = metrics.get('acc_norm,none', metrics.get('acc_norm', None))
                
                if acc is not None:
                    task_scores[task_name]['acc'].append(acc)
                if acc_norm is not None:
                    task_scores[task_name]['acc_norm'].append(acc_norm)
    
    # åˆ›å»ºå›¾è¡¨
    n_tasks = len(task_scores)
    
    if n_tasks == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»åŠ¡ç»“æœ")
        return
    
    # è®¾ç½®å›¾è¡¨å¤§å°
    fig, axes = plt.subplots(
        nrows=(n_tasks + 1) // 2,
        ncols=2,
        figsize=(15, 5 * ((n_tasks + 1) // 2))
    )
    
    if n_tasks == 1:
        axes = [axes]
    else:
        axes = axes.flatten()
    
    # ä¸ºæ¯ä¸ªä»»åŠ¡ç»˜åˆ¶å­å›¾
    for idx, (task_name, scores) in enumerate(sorted(task_scores.items())):
        ax = axes[idx]
        
        x = np.arange(len(checkpoints))
        width = 0.35
        
        # ç»˜åˆ¶ acc å’Œ acc_norm
        if scores['acc']:
            ax.bar(x - width/2, scores['acc'], width, label='ACC', alpha=0.8)
        if scores['acc_norm']:
            ax.bar(x + width/2, scores['acc_norm'], width, label='ACC (Normalized)', alpha=0.8)
        
        ax.set_xlabel('Checkpoint')
        ax.set_ylabel('Accuracy')
        ax.set_title(f'{task_name}')
        ax.set_xticks(x)
        ax.set_xticklabels(checkpoints, rotation=45, ha='right')
        ax.legend()
        ax.grid(True, alpha=0.3)
        ax.set_ylim([0, 1.0])
    
    # éšè—å¤šä½™çš„å­å›¾
    for idx in range(n_tasks, len(axes)):
        axes[idx].set_visible(False)
    
    plt.tight_layout()
    plt.savefig(output_file, dpi=150, bbox_inches='tight')
    print(f"âœ“ å›¾è¡¨å·²ä¿å­˜: {output_file}")
    plt.close()


def plot_average_performance(results, output_file="average_performance.png"):
    """ç»˜åˆ¶å¹³å‡æ€§èƒ½å›¾"""
    
    if not results:
        return
    
    checkpoints = []
    avg_acc = []
    avg_acc_norm = []
    
    for result in results:
        checkpoint = result['checkpoint']
        checkpoints.append(checkpoint)
        
        # è®¡ç®—å¹³å‡åˆ†æ•°
        acc_scores = []
        acc_norm_scores = []
        
        if 'results' in result['data']:
            for task_name, metrics in result['data']['results'].items():
                acc = metrics.get('acc,none', metrics.get('acc', None))
                acc_norm = metrics.get('acc_norm,none', metrics.get('acc_norm', None))
                
                if acc is not None:
                    acc_scores.append(acc)
                if acc_norm is not None:
                    acc_norm_scores.append(acc_norm)
        
        avg_acc.append(np.mean(acc_scores) if acc_scores else 0)
        avg_acc_norm.append(np.mean(acc_norm_scores) if acc_norm_scores else 0)
    
    # ç»˜åˆ¶å›¾è¡¨
    fig, ax = plt.subplots(figsize=(10, 6))
    
    x = np.arange(len(checkpoints))
    
    ax.plot(x, avg_acc, 'o-', label='Average ACC', linewidth=2, markersize=8)
    ax.plot(x, avg_acc_norm, 's-', label='Average ACC (Normalized)', linewidth=2, markersize=8)
    
    ax.set_xlabel('Checkpoint', fontsize=12)
    ax.set_ylabel('Average Accuracy', fontsize=12)
    ax.set_title('Average Performance Across All Tasks', fontsize=14, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(checkpoints, rotation=45, ha='right')
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)
    ax.set_ylim([0, 1.0])
    
    plt.tight_layout()
    plt.savefig(output_file, dpi=150, bbox_inches='tight')
    print(f"âœ“ å¹³å‡æ€§èƒ½å›¾å·²ä¿å­˜: {output_file}")
    plt.close()


def print_summary(results):
    """æ‰“å°ç»“æœæ‘˜è¦"""
    print("\n" + "="*80)
    print("ğŸ“Š è¯„ä¼°ç»“æœæ‘˜è¦")
    print("="*80)
    
    for result in results:
        checkpoint = result['checkpoint']
        print(f"\n{checkpoint}:")
        print("-" * 60)
        
        if 'results' in result['data']:
            for task_name, metrics in sorted(result['data']['results'].items()):
                acc = metrics.get('acc,none', metrics.get('acc', None))
                acc_norm = metrics.get('acc_norm,none', metrics.get('acc_norm', None))
                
                print(f"  {task_name:20s}", end="")
                if acc is not None:
                    print(f"  acc: {acc:.4f}", end="")
                if acc_norm is not None:
                    print(f"  acc_norm: {acc_norm:.4f}", end="")
                print()
    
    print("\n" + "="*80)


def main():
    print("\n" + "="*80)
    print("ğŸ“ˆ SmolLM2 è¯„ä¼°ç»“æœå¯è§†åŒ–")
    print("="*80 + "\n")
    
    # åŠ è½½ç»“æœ
    print("ğŸ“‚ åŠ è½½è¯„ä¼°ç»“æœ...")
    results = load_results()
    
    if not results:
        print("\nâŒ æ²¡æœ‰æ‰¾åˆ°è¯„ä¼°ç»“æœ")
        print("è¯·å…ˆè¿è¡Œè¯„ä¼°: bash run_evaluation.sh")
        return
    
    print(f"\nâœ“ æˆåŠŸåŠ è½½ {len(results)} ä¸ª checkpoint çš„ç»“æœ\n")
    
    # æ‰“å°æ‘˜è¦
    print_summary(results)
    
    # ç”Ÿæˆå›¾è¡¨
    print("\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    plot_comparison(results)
    plot_average_performance(results)
    
    print("\n" + "="*80)
    print("âœ… å®Œæˆ!")
    print("="*80 + "\n")


if __name__ == "__main__":
    main()

