# SmolLM2 è¯„ä¼°è¯¦ç»†æŒ‡å—

> **é€‚ç”¨å¯¹è±¡**: éœ€è¦æ·±å…¥äº†è§£æŠ€æœ¯ç»†èŠ‚ã€é‡åˆ°é—®é¢˜éœ€è¦æ’æŸ¥ã€æˆ–æƒ³æ‰©å±•é¡¹ç›®çš„ç”¨æˆ·

**æç¤º**: å¤§éƒ¨åˆ†æ“ä½œè¯·å‚è€ƒ [README.md](README.md)ï¼Œæœ¬æ–‡æ¡£ä»…ä¾›æ·±å…¥é˜…è¯»ã€‚

---

## ğŸ“‹ ç›®å½•

1. [æŠ€æœ¯æ¶æ„](#æŠ€æœ¯æ¶æ„)
2. [æ•°æ®æ¥æºè¯¦è§£](#æ•°æ®æ¥æºè¯¦è§£)
3. [é¢˜ç›®æ–‡ä»¶æ ¼å¼](#é¢˜ç›®æ–‡ä»¶æ ¼å¼)
4. [è¯„ä¼°å·¥ä½œåŸç†](#è¯„ä¼°å·¥ä½œåŸç†)
5. [æ•…éšœæ’æŸ¥](#æ•…éšœæ’æŸ¥)
6. [é«˜çº§ç”¨æ³•](#é«˜çº§ç”¨æ³•)
7. [é—®é¢˜è§£å†³å†å²](#é—®é¢˜è§£å†³å†å²)

---

## ğŸ—ï¸ æŠ€æœ¯æ¶æ„

### ç³»ç»Ÿç»„æˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        å®Œæ•´æŠ€æœ¯æ ˆ                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  1. è¯„ä¼°æ¡†æ¶ (lm-evaluation-harness)                          â”‚
â”‚     â€¢ ä½ç½®: /home2/yth/lm-evaluation-harness-competition/    â”‚
â”‚     â€¢ ç‰ˆæœ¬: 0.4.9                                             â”‚
â”‚     â€¢ åŠŸèƒ½: æä¾›æ ‡å‡†åŒ–çš„è¯„ä¼°æµç¨‹å’Œä»»åŠ¡å®šä¹‰                      â”‚
â”‚                                                               â”‚
â”‚  2. æ¨¡å‹ (SmolLM2-1.7B)                                       â”‚
â”‚     â€¢ æ¥æº: HuggingFace Hub                                   â”‚
â”‚     â€¢ å¤§å°: 3.42GB                                            â”‚
â”‚     â€¢ å‚æ•°é‡: 17äº¿                                            â”‚
â”‚     â€¢ æ£€æŸ¥ç‚¹: step-125000                                     â”‚
â”‚                                                               â”‚
â”‚  3. è¿è¡Œç¯å¢ƒ                                                  â”‚
â”‚     â€¢ Python: 3.12                                           â”‚
â”‚     â€¢ PyTorch: 2.9.0 + CUDA 12.8                            â”‚
â”‚     â€¢ GPU: 8å¼  (24GB æ˜¾å­˜/å¼ )                                â”‚
â”‚     â€¢ å†…å­˜: 314GB                                             â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ•°æ®æµ

```
[HuggingFace Hub]
     â†“ (ä¸‹è½½æ¨¡å‹å’Œæ•°æ®é›†)
[æœ¬åœ°ç¼“å­˜: ~/.cache/huggingface/]
     â†“ (åŠ è½½åˆ°å†…å­˜)
[GPU å†…å­˜]
     â†“ (è¿è¡Œæ¨ç†)
[è¯„ä¼°ç»“æœ]
     â†“ (ä¿å­˜)
[results/step-125000/*.json å’Œ *.jsonl]
     â†“ (å¯é€‰: æ‰“åŒ…æäº¤)
[submission.zip]
```

---

## ğŸ“¡ æ•°æ®æ¥æºè¯¦è§£

### æ‰€æœ‰æ•°æ®é›†çš„ HuggingFace é…ç½®

æ•°æ®é›†åœ°å€éƒ½åœ¨ `lm-evaluation-harness` çš„ YAML é…ç½®æ–‡ä»¶ä¸­é¢„å…ˆå®šä¹‰ï¼š

| æ•°æ®é›† | é…ç½®æ–‡ä»¶è·¯å¾„ | dataset_path | HuggingFace åœ°å€ |
|--------|-------------|--------------|-----------------|
| HellaSwag | `lm_eval/tasks/hellaswag/hellaswag.yaml` | `hellaswag` | https://huggingface.co/datasets/hellaswag |
| ARC | `lm_eval/tasks/arc/arc_easy.yaml` | `allenai/ai2_arc` | https://huggingface.co/datasets/allenai/ai2_arc |
| MMLU | `lm_eval/tasks/mmlu/default/_default_template_yaml` | `cais/mmlu` | https://huggingface.co/datasets/cais/mmlu |
| PIQA | `lm_eval/tasks/piqa/piqa.yaml` | `baber/piqa` | https://huggingface.co/datasets/baber/piqa |
| WinoGrande | `lm_eval/tasks/winogrande/default.yaml` | `winogrande` | https://huggingface.co/datasets/winogrande |
| OpenBookQA | `lm_eval/tasks/openbookqa/openbookqa.yaml` | `openbookqa` | https://huggingface.co/datasets/openbookqa |
| BoolQ | å†…ç½®é…ç½® | `boolq` | https://huggingface.co/datasets/boolq |

### æ•°æ®ä¸‹è½½æµç¨‹

```python
# lm_eval å†…éƒ¨å·¥ä½œæµç¨‹ï¼ˆç®€åŒ–ï¼‰

from datasets import load_dataset

# 1. è¯»å– YAML é…ç½®
task_config = yaml.load("hellaswag.yaml")
dataset_path = task_config['dataset_path']  # "hellaswag"

# 2. ä» HuggingFace ä¸‹è½½/åŠ è½½æ•°æ®é›†
dataset = load_dataset(
    dataset_path,           # "hellaswag"
    split="validation",     # ä½¿ç”¨éªŒè¯é›†
    cache_dir="~/.cache/huggingface/datasets/"
)

# 3. å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°
for sample in dataset:
    prompt = format_prompt(sample)
    answer = model.generate(prompt)
    score = evaluate(answer, sample['correct_answer'])
```

### ç¼“å­˜ä½ç½®

```bash
# æ¨¡å‹ç¼“å­˜
~/.cache/huggingface/hub/
â””â”€â”€ models--HuggingFaceTB--SmolLM2-1.7B-intermediate-checkpoints/
    â””â”€â”€ snapshots/
        â””â”€â”€ [revision-hash]/
            â”œâ”€â”€ model.safetensors  (3.42GB)
            â”œâ”€â”€ config.json
            â””â”€â”€ tokenizer.json

# æ•°æ®é›†ç¼“å­˜
~/.cache/huggingface/datasets/
â”œâ”€â”€ hellaswag/              (180 MB)
â”œâ”€â”€ allenai___ai2_arc/      (30 MB)
â”œâ”€â”€ cais___mmlu/            (250 MB)
â”œâ”€â”€ baber___piqa/           (10 MB)
â”œâ”€â”€ winogrande/             (5 MB)
â”œâ”€â”€ openbookqa/             (5 MB)
â””â”€â”€ super_glue/             (10 MB, åŒ…å« BoolQ)

æ€»è®¡: ~490 MB
```

---

## ğŸ“„ é¢˜ç›®æ–‡ä»¶æ ¼å¼

### JSON Lines æ ¼å¼

æ¯ä¸ª `samples_*.jsonl` æ–‡ä»¶åŒ…å«å¤šè¡Œï¼Œæ¯è¡Œæ˜¯ä¸€ä¸ªå®Œæ•´çš„ JSON å¯¹è±¡ï¼ˆä¸€ä¸ªé¢˜ç›®ï¼‰ï¼š

```jsonl
{"doc_id": 0, "doc": {...}, "target": "2", "resps": [...], ...}
{"doc_id": 1, "doc": {...}, "target": "1", "resps": [...], ...}
{"doc_id": 2, "doc": {...}, "target": "3", "resps": [...], ...}
```

### å®Œæ•´å­—æ®µè¯´æ˜

```json
{
    "doc_id": 0,                              // é¢˜ç›®ç¼–å·
    "doc": {                                  // åŸå§‹é¢˜ç›®æ•°æ®
        "id": "Mercury_7175875",              // æ•°æ®é›†ä¸­çš„ ID
        "question": "é¢˜ç›®æ–‡æœ¬...",
        "choices": {
            "text": ["é€‰é¡¹A", "é€‰é¡¹B", "é€‰é¡¹C", "é€‰é¡¹D"],
            "label": ["A", "B", "C", "D"]
        },
        "answerKey": "C"                      // æ­£ç¡®ç­”æ¡ˆæ ‡ç­¾
    },
    "target": "2",                            // æ­£ç¡®ç­”æ¡ˆç´¢å¼•ï¼ˆ0-basedï¼‰
    "arguments": {                            // æ¨¡å‹çœ‹åˆ°çš„å®Œæ•´æç¤º
        "gen_args_0": {
            "arg_0": "Question: ...\nAnswer:",
            "arg_1": " é€‰é¡¹Aæ–‡æœ¬"
        },
        "gen_args_1": {...},
        "gen_args_2": {...},
        "gen_args_3": {...}
    },
    "resps": [                                // æ¨¡å‹åŸå§‹å“åº”
        [[-18.875, false]],                   // é€‰é¡¹ A çš„åˆ†æ•°
        [[-25.125, false]],                   // é€‰é¡¹ B çš„åˆ†æ•°
        [[-21.875, false]],                   // é€‰é¡¹ C çš„åˆ†æ•°ï¼ˆæ­£ç¡®ï¼‰
        [[-15.250, false]]                    // é€‰é¡¹ D çš„åˆ†æ•°ï¼ˆæ¨¡å‹é€‰æ‹©ï¼‰
    ],
    "filtered_resps": [                       // å¤„ç†åçš„åˆ†æ•°
        [-18.875],
        [-25.125],
        [-21.875],
        [-15.250]
    ],
    "doc_hash": "...",                        // æ–‡æ¡£å“ˆå¸Œï¼ˆå»é‡ï¼‰
    "prompt_hash": "...",                     // æç¤ºå“ˆå¸Œ
    "target_hash": "..."                      // ç›®æ ‡å“ˆå¸Œ
}
```

### åˆ†æ•°å«ä¹‰

- **è´Ÿæ•°è¶Šå°ï¼ˆç»å¯¹å€¼è¶Šå¤§ï¼‰= è¶Šä¸å¯èƒ½**
- **è´Ÿæ•°è¶Šå¤§ï¼ˆè¶Šæ¥è¿‘ 0ï¼‰= è¶Šå¯èƒ½**

ç¤ºä¾‹:
```
-15.250  â† æœ€é«˜åˆ†ï¼Œæ¨¡å‹é€‰æ‹©è¿™ä¸ª
-18.875  â† ç¬¬äºŒé«˜åˆ†
-21.875  â† æ­£ç¡®ç­”æ¡ˆï¼ˆç¬¬ä¸‰ï¼‰
-25.125  â† æœ€ä½åˆ†
```

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæ¨¡å‹é€‰é”™äº†ï¼ˆé€‰äº† -15.250ï¼Œè€Œæ­£ç¡®ç­”æ¡ˆæ˜¯ -21.875ï¼‰ã€‚

---

## âš™ï¸ è¯„ä¼°å·¥ä½œåŸç†

### lm-evaluation-harness æµç¨‹

```python
# 1. åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "HuggingFaceTB/SmolLM2-1.7B-intermediate-checkpoints",
    revision="step-125000",
    torch_dtype=torch.bfloat16
)
tokenizer = AutoTokenizer.from_pretrained(...)

# 2. åŠ è½½æ•°æ®é›†
dataset = load_dataset("hellaswag", split="validation")

# 3. å¯¹æ¯ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°
results = []
for sample in dataset:
    # æ„å»º 4 ä¸ªå®Œæ•´æç¤ºï¼ˆé¢˜ç›® + æ¯ä¸ªé€‰é¡¹ï¼‰
    prompts = [
        f"{sample['query']} {choice}"
        for choice in sample['endings']
    ]
    
    # è®¡ç®—æ¯ä¸ªæç¤ºçš„å¯¹æ•°æ¦‚ç‡
    log_probs = []
    for prompt in prompts:
        tokens = tokenizer(prompt, return_tensors="pt")
        with torch.no_grad():
            outputs = model(**tokens)
            logits = outputs.logits
            # è®¡ç®—å¯¹æ•°æ¦‚ç‡
            log_prob = compute_log_prob(logits, tokens)
            log_probs.append(log_prob)
    
    # é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„é€‰é¡¹
    predicted = log_probs.index(max(log_probs))
    correct = int(sample['label'])
    
    results.append({
        'predicted': predicted,
        'correct': correct,
        'is_correct': predicted == correct
    })

# 4. è®¡ç®—å‡†ç¡®ç‡
accuracy = sum(r['is_correct'] for r in results) / len(results)
```

### å½’ä¸€åŒ–å‡†ç¡®ç‡è®¡ç®—

**é—®é¢˜**: å¤š token ç­”æ¡ˆä¼šå› ä¸ºæ¦‚ç‡ç›¸ä¹˜è€Œå¾—åˆ†æ›´ä½

```python
# ä¾‹å­
answer_1 = "Dubai"        # 1 token
answer_2 = "Abu Dhabi"    # 2 tokens ["Abu", "Dhabi"]

# æœªå½’ä¸€åŒ–
P(Dubai) = 0.8
P(Abu Dhabi) = 0.7 * 0.6 = 0.42

# Dubai å¾—åˆ†æ›´é«˜ï¼Œä½†è¿™ä¸å…¬å¹³ï¼

# å½’ä¸€åŒ–ï¼šå¹³å‡æ¯ä¸ª token çš„å¯¹æ•°æ¦‚ç‡
log_P_norm(Dubai) = log(0.8) / 1 = -0.097
log_P_norm(Abu Dhabi) = log(0.42) / 2 = -0.434

# ç°åœ¨å¯ä»¥å…¬å¹³æ¯”è¾ƒäº†
```

**å®ç°**:
```python
def normalized_accuracy(log_probs, num_tokens):
    """å½’ä¸€åŒ–å‡†ç¡®ç‡è®¡ç®—"""
    return log_probs / num_tokens
```

---

## ğŸ”§ æ•…éšœæ’æŸ¥

### å¸¸è§é”™è¯¯åŠè§£å†³

#### 1. SafetensorError

**é”™è¯¯ä¿¡æ¯**:
```
safetensors_rust.SafetensorError: 
Error while deserializing header: 
invalid JSON in header: control character found
```

**åŸå› **: æ¨¡å‹æ–‡ä»¶ä¸‹è½½ä¸å®Œæ•´æˆ–æŸå

**è§£å†³**:
```bash
# 1. æ¸…é™¤æŸåç¼“å­˜
rm -rf ~/.cache/huggingface/hub/models--HuggingFaceTB*

# 2. è®¾ç½®é•œåƒæº
export HF_ENDPOINT=https://hf-mirror.com

# 3. é‡æ–°è¿è¡Œè¯„ä¼°
cd /home2/yth/smollm2_evaluation
bash run_evaluation.sh
```

#### 2. CUDA Out of Memory

**é”™è¯¯ä¿¡æ¯**:
```
torch.cuda.OutOfMemoryError: CUDA out of memory
```

**è§£å†³æ–¹æ¡ˆ**:

```bash
# æ–¹æ³• 1: ä½¿ç”¨æ›´å°çš„ batch size
lm_eval --batch_size 4 ...

# æ–¹æ³• 2: ä½¿ç”¨ float16
lm_eval --model_args dtype=float16 ...

# æ–¹æ³• 3: ä½¿ç”¨ CPUï¼ˆæ…¢ï¼‰
lm_eval --device cpu ...

# æ–¹æ³• 4: åˆ†ä»»åŠ¡è¯„ä¼°
lm_eval --tasks hellaswag ...  # ä¸€æ¬¡ä¸€ä¸ªä»»åŠ¡
```

#### 3. ä¸‹è½½é€Ÿåº¦æ…¢

**ç—‡çŠ¶**: ä¸‹è½½åªæœ‰å‡ å KB/s

**è§£å†³**:
```bash
# ä½¿ç”¨å›½å†…é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com

# æˆ–ä½¿ç”¨ä»£ç†
export http_proxy=http://proxy.example.com:8080
export https_proxy=http://proxy.example.com:8080
```

#### 4. æ•°æ®é›†åŠ è½½å¤±è´¥

**é”™è¯¯**: `DatasetNotFoundError`

**è§£å†³**:
```bash
# æ‰‹åŠ¨é¢„ä¸‹è½½æ•°æ®é›†
python3 << EOF
from datasets import load_dataset
load_dataset("hellaswag")
load_dataset("allenai/ai2_arc", "ARC-Easy")
load_dataset("cais/mmlu", "abstract_algebra")
EOF
```

#### 5. é‡å¤è­¦å‘Šä¿¡æ¯

**ç°è±¡**: `trust_remote_code is not supported anymore` é‡å¤å‡ºç°

**è§£é‡Š**: è¿™æ˜¯æ­£å¸¸çš„è­¦å‘Šï¼Œä¸å½±å“è¯„ä¼°ã€‚MMLU æœ‰ 57 ä¸ªå­ä»»åŠ¡ï¼Œæ¯ä¸ªéƒ½ä¼šæ˜¾ç¤ºä¸€æ¬¡ã€‚

**æ— éœ€å¤„ç†**ï¼Œå¯ä»¥å®‰å…¨å¿½ç•¥ã€‚

---

## ğŸš€ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰ä»»åŠ¡é…ç½®

åˆ›å»ºè‡ªå®šä¹‰ä»»åŠ¡ YAML æ–‡ä»¶:

```yaml
# custom_task.yaml
task: my_custom_task
dataset_path: path/to/dataset
output_type: multiple_choice
doc_to_text: "Question: {{question}}\nAnswer:"
doc_to_target: "{{answer}}"
doc_to_choice: "{{choices}}"
metric_list:
  - metric: acc
    aggregation: mean
    higher_is_better: true
```

ä½¿ç”¨è‡ªå®šä¹‰ä»»åŠ¡:
```bash
lm_eval --model hf \
    --model_args pretrained=... \
    --tasks custom_task \
    --include_path /path/to/custom_task.yaml
```

### æ‰¹é‡è¯„ä¼°å¤šä¸ªæ¨¡å‹

```bash
#!/bin/bash

MODELS=(
    "HuggingFaceTB/SmolLM2-135M"
    "HuggingFaceTB/SmolLM2-360M"
    "HuggingFaceTB/SmolLM2-1.7B"
)

for model in "${MODELS[@]}"; do
    echo "Evaluating $model..."
    lm_eval --model hf \
        --model_args pretrained=$model \
        --tasks hellaswag,arc_easy,mmlu \
        --output_path results/$(basename $model)/
done
```

### ä½¿ç”¨ Few-shot è¯„ä¼°

```bash
# 5-shot è¯„ä¼°ï¼ˆæä¾› 5 ä¸ªç¤ºä¾‹ï¼‰
lm_eval --model hf \
    --model_args pretrained=... \
    --tasks hellaswag \
    --num_fewshot 5
```

### ä¿å­˜è¯¦ç»†æ—¥å¿—

```bash
lm_eval --model hf \
    --model_args pretrained=... \
    --tasks hellaswag \
    --log_samples \                    # ä¿å­˜æ‰€æœ‰æ ·æœ¬
    --output_path results/ \
    --verbosity DEBUG > eval.log 2>&1  # ä¿å­˜è¯¦ç»†æ—¥å¿—
```

---

## ğŸ“Š æ•°æ®é›†è¯¦ç»†ä¿¡æ¯

### MMLU çš„ 57 ä¸ªå­å­¦ç§‘

#### äººæ–‡å­¦ç§‘ (13ä¸ª)
- formal_logic, high_school_european_history, high_school_us_history
- high_school_world_history, international_law, jurisprudence
- logical_fallacies, moral_disputes, moral_scenarios
- philosophy, prehistory, professional_law, world_religions

#### ç¤¾ä¼šç§‘å­¦ (12ä¸ª)
- econometrics, high_school_geography, high_school_government_and_politics
- high_school_macroeconomics, high_school_microeconomics, high_school_psychology
- human_sexuality, professional_psychology, public_relations
- security_studies, sociology, us_foreign_policy

#### STEM (24ä¸ª)
- abstract_algebra, anatomy, astronomy, college_biology
- college_chemistry, college_computer_science, college_mathematics, college_physics
- computer_security, conceptual_physics, electrical_engineering, elementary_mathematics
- high_school_biology, high_school_chemistry, high_school_computer_science
- high_school_mathematics, high_school_physics, high_school_statistics
- machine_learning, ...

#### å…¶ä»– (8ä¸ª)
- business_ethics, clinical_knowledge, college_medicine, global_facts
- human_aging, management, marketing, medical_genetics
- miscellaneous, nutrition, professional_accounting, professional_medicine, virology

### æŸ¥çœ‹ MMLU å„å­¦ç§‘è¡¨ç°

```bash
cd /home2/yth/smollm2_evaluation

python3 << 'EOF'
import json
import glob

# è¯»å–ç»“æœæ–‡ä»¶
result_files = glob.glob('results/step-125000/**/results_*.json', recursive=True)
with open(result_files[0], 'r') as f:
    data = json.load(f)

# æå– MMLU å­ä»»åŠ¡
mmlu_tasks = {k: v for k, v in data['results'].items() 
              if k.startswith('mmlu_') and k != 'mmlu'}

# æŒ‰å‡†ç¡®ç‡æ’åº
sorted_tasks = sorted(mmlu_tasks.items(), 
                     key=lambda x: x[1].get('acc,none', 0), 
                     reverse=True)

print("MMLU å­ä»»åŠ¡æ€§èƒ½æ’å (Top 10):")
print("=" * 70)
for i, (task, metrics) in enumerate(sorted_tasks[:10], 1):
    acc = metrics.get('acc,none', 0)
    task_name = task.replace('mmlu_', '').replace('_', ' ').title()
    print(f"{i:2}. {task_name:40} {acc:.2%}")
EOF
```

---

## ğŸ› ï¸ é—®é¢˜è§£å†³å†å²

### é‡åˆ°çš„é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

#### é—®é¢˜ 1: ä¸‹è½½é€Ÿåº¦æ…¢ (122 KB/s)

**æ—¶é—´**: 2025-10-19 11:30

**å‘ç°**:
```
model.safetensors: [è¿›åº¦æ¡] 122kB/s
é¢„è®¡æ—¶é—´: 8+ å°æ—¶
```

**åŸå› **: ç›´è¿ HuggingFace æœåŠ¡å™¨ï¼ˆå›½å¤–ï¼‰ï¼Œå›½é™…å¸¦å®½é™åˆ¶

**è§£å†³**:
```bash
export HF_ENDPOINT=https://hf-mirror.com
# é€Ÿåº¦æå‡åˆ° 5-20 MB/s (æå‡ 40-160 å€)
```

**æ•ˆæœ**: ä¸‹è½½æ—¶é—´ä» 8 å°æ—¶é™è‡³ 3-12 åˆ†é’Ÿ

#### é—®é¢˜ 2: SafetensorError

**æ—¶é—´**: 2025-10-19 11:45

**é”™è¯¯**:
```python
safetensors_rust.SafetensorError: 
Error while deserializing header: 
invalid JSON in header: control character found
```

**åŸå› **: ä¹‹å‰ä¸‹è½½ä¸­æ–­ï¼Œç¼“å­˜äº† 810MB (23.7%) çš„ä¸å®Œæ•´æ–‡ä»¶

**è§£å†³**:
```bash
# æ¸…é™¤æŸåç¼“å­˜
rm -rf ~/.cache/huggingface/hub/models--HuggingFaceTB*

# ä½¿ç”¨é•œåƒé‡æ–°ä¸‹è½½
export HF_ENDPOINT=https://hf-mirror.com
bash run_evaluation.sh
```

**ç»éªŒæ•™è®­**:
- ä¸‹è½½å¤§æ–‡ä»¶å‰å…ˆè®¾ç½®é•œåƒæº
- ä¸è¦ä¸­æ–­ä¸‹è½½
- å¦‚æœä¸­æ–­äº†ï¼Œå…ˆæ¸…é™¤ç¼“å­˜å†é‡è¯•

#### é—®é¢˜ 3: é‡å¤è­¦å‘Š

**æ—¶é—´**: 2025-10-19 12:20

**ç°è±¡**:
```
`trust_remote_code` is not supported anymore.
... (é‡å¤ 57 æ¬¡)
```

**è§£é‡Š**: MMLU æœ‰ 57 ä¸ªå­ä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡åŠ è½½æ—¶éƒ½ä¼šæ˜¾ç¤ºè­¦å‘Š

**ç»“è®º**: è¿™æ˜¯æ­£å¸¸ç°è±¡ï¼Œä¸å½±å“è¯„ä¼°ç»“æœï¼Œå¯ä»¥å®‰å…¨å¿½ç•¥

---

## ğŸ“š å‚è€ƒèµ„æº

### è®ºæ–‡

- **HellaSwag**: [Can a Machine Really Finish Your Sentence?](https://arxiv.org/abs/1905.07830)
- **ARC**: [Think you have Solved Question Answering?](https://arxiv.org/abs/1803.05457)
- **MMLU**: [Measuring Massive Multitask Language Understanding](https://arxiv.org/abs/2009.03300)
- **PIQA**: [Physical Interaction QA](https://arxiv.org/abs/1911.11641)
- **WinoGrande**: [An Adversarial Winograd Schema Challenge](https://arxiv.org/abs/1907.10641)

### åœ¨çº¿èµ„æº

- [LM Evaluation Harness æ–‡æ¡£](https://github.com/EleutherAI/lm-evaluation-harness)
- [HuggingFace Transformers](https://huggingface.co/docs/transformers)
- [é›†æˆåŸºå‡†æµ‹è¯•æŒ‡å—](https://huggingface.co/blog/integrating-benchmarks-lm-eval)

---

**æœ€åæ›´æ–°**: 2025-10-19  
**ç»´æŠ¤è€…**: AI Assistant  
**åé¦ˆ**: å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·å‚è€ƒ README.md æˆ–æŸ¥çœ‹é¡¹ç›®æ–‡æ¡£

