#!/usr/bin/env python3
"""
Evaluate Dense-3B Model on Custom Fill-in-the-blank MMLU Dataset - Batch Version
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_from_disk
from tqdm import tqdm
import json
import os
import argparse
from datetime import datetime
import re

def load_model_and_tokenizer(model_name, subfolder=None, dtype="bfloat16"):
    """åŠ è½½æ¨¡å‹å’Œtokenizer"""
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
    if subfolder:
        print(f"  Subfolder: {subfolder}")
    
    # æ£€æŸ¥GPU
    if torch.cuda.is_available():
        print(f"  GPU: {torch.cuda.get_device_name(0)}")
        print(f"  GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    else:
        print("  âš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUï¼ˆä¼šéå¸¸æ…¢ï¼‰")
    
    # è®¾ç½®dtype
    dtype_map = {
        "bfloat16": torch.bfloat16,
        "float16": torch.float16,
        "float32": torch.float32,
    }
    torch_dtype = dtype_map.get(dtype, torch.bfloat16)
    print(f"  æ•°æ®ç±»å‹: {dtype}")
    
    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        subfolder=subfolder,
        trust_remote_code=True
    )
    
    # è®¾ç½®pad_tokenå’Œpaddingæ–¹å‘
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = 'left'  # decoderæ¨¡å‹å¿…é¡»å·¦padding
    
    print(f"  Tokenizer padding side: {tokenizer.padding_side}")
    
    # åŠ è½½æ¨¡å‹
    print("  æ­£åœ¨åŠ è½½æ¨¡å‹æƒé‡...")
    device_map = {"": 0}  # ä½¿ç”¨GPU 0
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        subfolder=subfolder,
        torch_dtype=torch_dtype,
        device_map=device_map,
        trust_remote_code=True
    )
    
    model.eval()
    print(f"  âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
    
    return model, tokenizer

def generate_answers_batch(model, tokenizer, prompts, max_new_tokens=10):
    """æ‰¹å¤„ç†ç”Ÿæˆç­”æ¡ˆ"""
    # Tokenizeæ‰€æœ‰prompts
    inputs = tokenizer(
        prompts,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=512
    )
    
    input_ids = inputs['input_ids'].to(model.device)
    attention_mask = inputs['attention_mask'].to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_new_tokens=max_new_tokens,
            do_sample=False,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
    generated_texts = []
    for i, output in enumerate(outputs):
        # åªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
        generated = tokenizer.decode(
            output[input_ids.shape[1]:],
            skip_special_tokens=True
        )
        generated_texts.append(generated.strip())
    
    return generated_texts

def normalize_answer(text):
    """æ ‡å‡†åŒ–ç­”æ¡ˆæ–‡æœ¬"""
    text = re.sub(r'\s+', ' ', text)
    text = text.lower().strip()
    text = re.sub(r'[.,;:!?]', '', text)
    return text

def check_answer(generated, reference):
    """æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦æ­£ç¡®"""
    # ç²¾ç¡®åŒ¹é…
    if normalize_answer(generated) == normalize_answer(reference):
        return True, "exact_match"
    
    # åŒ…å«åŒ¹é…
    if normalize_answer(reference) in normalize_answer(generated):
        return True, "contains"
    
    # æ•°å­—åŒ¹é…
    try:
        ref_num = float(reference.replace(',', ''))
        numbers = re.findall(r'-?\d+\.?\d*', generated.replace(',', ''))
        if numbers and abs(float(numbers[0]) - ref_num) < 0.01:
            return True, "numeric_match"
    except:
        pass
    
    return False, "no_match"

def evaluate_dataset(model, tokenizer, dataset, batch_size=16, max_samples=None, max_new_tokens=10):
    """è¯„ä¼°æ•°æ®é›† - æ‰¹å¤„ç†ç‰ˆæœ¬"""
    results = {
        "total": 0,
        "correct": 0,
        "exact_match": 0,
        "contains_match": 0,
        "numeric_match": 0,
        "by_subject": {},
        "samples": []
    }
    
    data = dataset['test']
    if max_samples:
        data = data.select(range(min(max_samples, len(data))))
    
    print(f"\nå¼€å§‹è¯„ä¼° {len(data)} ä¸ªæ ·æœ¬...")
    print(f"æ‰¹å¤„ç†å¤§å°: {batch_size}")
    print(f"æœ€å¤§ç”Ÿæˆé•¿åº¦: {max_new_tokens} tokens")
    
    if torch.cuda.is_available():
        print(f"\nGPU: {torch.cuda.get_device_name(0)}")
        print(f"åˆå§‹GPUå†…å­˜: {torch.cuda.memory_allocated() / 1024**3:.2f} GB / {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    import time
    start_time = time.time()
    
    # æ‰¹å¤„ç†è¯„ä¼°
    num_batches = (len(data) + batch_size - 1) // batch_size
    
    for batch_idx in tqdm(range(num_batches), desc="æ‰¹å¤„ç†è¿›åº¦"):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, len(data))
        
        # å‡†å¤‡æ‰¹æ¬¡prompts
        prompts = []
        batch_info = []  # å­˜å‚¨æ¯ä¸ªæ ·æœ¬çš„ä¿¡æ¯
        
        for i in range(start_idx, end_idx):
            item = data[i]
            prompt = f"Question: {item['fill_blank_question']}\nAnswer:"
            prompts.append(prompt)
            batch_info.append({
                'question': item['fill_blank_question'],
                'answer': item['fill_blank_answer'],
                'subject': item['subject'],
                'index': i
            })
        
        # æ‰¹é‡ç”Ÿæˆ
        generated_answers = generate_answers_batch(model, tokenizer, prompts, max_new_tokens)
        
        # æ£€æŸ¥ç­”æ¡ˆ
        for i, info in enumerate(batch_info):
            reference_answer = info['answer']
            generated_answer = generated_answers[i]
            subject = info['subject']
            
            is_correct, match_type = check_answer(generated_answer, reference_answer)
            
            # æ›´æ–°ç»Ÿè®¡
            results["total"] += 1
            if is_correct:
                results["correct"] += 1
                if match_type == "exact_match":
                    results["exact_match"] += 1
                elif match_type == "contains":
                    results["contains_match"] += 1
                elif match_type == "numeric_match":
                    results["numeric_match"] += 1
            
            # æŒ‰å­¦ç§‘ç»Ÿè®¡
            if subject not in results["by_subject"]:
                results["by_subject"][subject] = {"total": 0, "correct": 0}
            results["by_subject"][subject]["total"] += 1
            if is_correct:
                results["by_subject"][subject]["correct"] += 1
            
            # ä¿å­˜æ ·æœ¬
            sample_result = {
                "index": info['index'],
                "subject": subject,
                "question": info['question'],
                "reference_answer": reference_answer,
                "generated_answer": generated_answer,
                "is_correct": is_correct,
                "match_type": match_type
            }
            results["samples"].append(sample_result)
    
    # è®¡ç®—ç»Ÿè®¡
    results["accuracy"] = results["correct"] / results["total"] if results["total"] > 0 else 0
    
    for subject, stats in results["by_subject"].items():
        stats["accuracy"] = stats["correct"] / stats["total"] if stats["total"] > 0 else 0
    
    elapsed_time = time.time() - start_time
    results["elapsed_time"] = elapsed_time
    results["samples_per_second"] = results["total"] / elapsed_time if elapsed_time > 0 else 0
    
    print(f"\nè¯„ä¼°å®Œæˆ!")
    print(f"æ€»è€—æ—¶: {elapsed_time / 60:.1f} åˆ†é’Ÿ ({elapsed_time:.1f} ç§’)")
    print(f"å¹³å‡é€Ÿåº¦: {results['samples_per_second']:.2f} æ ·æœ¬/ç§’")
    print(f"ğŸš€ æé€Ÿæ•ˆæœ: æ‰¹å¤„ç† vs å•ä¸ª = {results['samples_per_second'] / 0.5:.1f}x å€")
    
    if torch.cuda.is_available():
        print(f"æœ€ç»ˆGPUå†…å­˜: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
    
    return results

def save_results(results, output_path):
    """ä¿å­˜ç»“æœ"""
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"\nç»“æœå·²ä¿å­˜åˆ°: {output_path}")

def print_results(results):
    """æ‰“å°ç»“æœ"""
    print("\n" + "="*70)
    print("è¯„ä¼°ç»“æœ")
    print("="*70)
    print(f"æ€»æ ·æœ¬æ•°: {results['total']}")
    print(f"æ­£ç¡®æ•°é‡: {results['correct']}")
    print(f"å‡†ç¡®ç‡: {results['accuracy']:.2%}")
    print(f"\nåŒ¹é…ç±»å‹åˆ†å¸ƒ:")
    print(f"  ç²¾ç¡®åŒ¹é…: {results['exact_match']}")
    print(f"  åŒ…å«åŒ¹é…: {results['contains_match']}")
    print(f"  æ•°å­—åŒ¹é…: {results['numeric_match']}")
    
    print(f"\næŒ‰å­¦ç§‘å‡†ç¡®ç‡ (Top 10):")
    sorted_subjects = sorted(
        results["by_subject"].items(),
        key=lambda x: x[1]["accuracy"],
        reverse=True
    )[:10]
    
    for subject, stats in sorted_subjects:
        print(f"  {subject:40s}: {stats['accuracy']:.2%} ({stats['correct']}/{stats['total']})")
    
    print("="*70)

def main():
    parser = argparse.ArgumentParser(description="Evaluate Fill-in-the-blank MMLU - Batch Version")
    parser.add_argument("--model_name", type=str, default="tiiuae/dense-3b-arch1")
    parser.add_argument("--subfolder", type=str, default=None)
    parser.add_argument("--dataset_path", type=str,
                        default="/home2/yth/.cache/huggingface/datasets/mmlu_fill_blank_dataset_8500/test")
    parser.add_argument("--output_dir", type=str, default="results_fillblank_batch")
    parser.add_argument("--dtype", type=str, default="bfloat16",
                        choices=["bfloat16", "float16", "float32"])
    parser.add_argument("--batch_size", type=int, default=16,
                        help="æ‰¹å¤„ç†å¤§å°ï¼ˆå»ºè®®8-32ï¼‰")
    parser.add_argument("--max_samples", type=int, default=None)
    parser.add_argument("--max_new_tokens", type=int, default=20,
                        help="ç”Ÿæˆçš„æœ€å¤§tokenæ•°ï¼ˆå»ºè®®10-30ï¼‰")
    
    args = parser.parse_args()
    
    print("="*70)
    print("ğŸš€ Fill-in-the-blank Evaluation (Batch Version)")
    print("="*70)
    print(f"Model: {args.model_name}")
    print(f"Batch size: {args.batch_size}")
    print(f"Max new tokens: {args.max_new_tokens}")
    print("="*70)
    
    # åŠ è½½æ•°æ®é›†
    print("\næ­£åœ¨åŠ è½½æ•°æ®é›†...")
    dataset = load_from_disk(args.dataset_path)
    print(f"æ•°æ®é›†å·²åŠ è½½: {len(dataset['test'])} ä¸ªæ ·æœ¬")
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer = load_model_and_tokenizer(
        args.model_name,
        args.subfolder,
        args.dtype
    )
    
    # è¯„ä¼°
    results = evaluate_dataset(
        model,
        tokenizer,
        dataset,
        batch_size=args.batch_size,
        max_samples=args.max_samples,
        max_new_tokens=args.max_new_tokens
    )
    
    # æ‰“å°ç»“æœ
    print_results(results)
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    checkpoint_name = args.subfolder if args.subfolder else "base"
    output_path = os.path.join(
        args.output_dir,
        checkpoint_name,
        f"results_{timestamp}.json"
    )
    save_results(results, output_path)
    
    # æ‰“å°ç¤ºä¾‹
    if results["samples"]:
        print("\n" + "="*70)
        print("ç¤ºä¾‹ç»“æœ (å‰5ä¸ª):")
        print("="*70)
        for sample in results["samples"][:5]:
            print(f"\né—®é¢˜: {sample['question'][:80]}...")
            print(f"å‚è€ƒç­”æ¡ˆ: {sample['reference_answer']}")
            print(f"ç”Ÿæˆç­”æ¡ˆ: {sample['generated_answer']}")
            print(f"æ˜¯å¦æ­£ç¡®: {'âœ“' if sample['is_correct'] else 'âœ—'} ({sample['match_type']})")
            print("-" * 70)

if __name__ == "__main__":
    main()

